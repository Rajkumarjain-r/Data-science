{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Q2: List down techniques used to handle missing data.  Give an example of each with python code.\n",
    "\n",
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required.\n",
    "\n",
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9fa37",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "\n",
    "Missing values are null fields in a dataset. If they are not handled the noise will be introduced in the model and model performance will not be good. \n",
    "\n",
    "Missing values in a dataset refer to the absence of data for one or more variables in some observations. There are various reasons why missing values can occur, such as errors in data collection, data entry issues, non-response from survey participants, or data corruption during storage or transmission.\n",
    "\n",
    "Handling missing values is essential because they can lead to biased or incorrect analysis and modeling results. If missing values are not handled properly, it can introduce errors and affect the accuracy of statistical analysis and machine learning models. Some reasons why it is important to handle missing values include:\n",
    "\n",
    "Accurate analysis: Missing values can distort statistical summaries and distributions, leading to incorrect conclusions.\n",
    "\n",
    "Reliable models: Many machine learning algorithms cannot handle missing values, so they require imputation (filling in the missing values) or other treatments to be used effectively.\n",
    "\n",
    "Preventing bias: Missing data is often not random but occurs systematically, which can introduce bias in the analysis or models if not handled properly.\n",
    "\n",
    "Maintaining data integrity: Missing values can lead to problems in data integration, data mining, and other data-related tasks.\n",
    "\n",
    "Ethical concerns: In some cases, missing data can represent sensitive or private information, and appropriate handling is crucial to ensure data privacy and security.\n",
    "\n",
    "Algorithms that are not affected by missing values include:\n",
    "\n",
    "Decision Trees: Decision trees can handle missing values by effectively treating missing data as just another value during the tree-building process.\n",
    "\n",
    "Random Forest: As an ensemble of decision trees, random forests can also handle missing values without requiring pre-processing.\n",
    "\n",
    "Gradient Boosting Machines (GBM): GBM algorithms like XGBoost and LightGBM can handle missing values by making appropriate splits during the boosting process.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN can work with missing values by ignoring missing features during distance calculations.\n",
    "\n",
    "Gaussian Mixture Models (GMM): GMM is often robust to some extent of missing data.\n",
    "\n",
    "It's essential to note that while some algorithms can handle missing values inherently, it is still generally good practice to handle missing values appropriately before applying any machine learning algorithm to ensure the best possible results. Different imputation techniques, such as mean imputation, median imputation, regression imputation, or advanced methods like Multiple Imputation, can be used to fill in missing values based on the characteristics of the data and the specific problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe60ac",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data.  Give an example of each with python code.\n",
    "  \n",
    "In case of contionus data with normal distribution, then null is replaced by mean\n",
    "In case of continous data with left skewed or right skewed distribution then relace null with meadian is good\n",
    "In case of categorical data replace null with mode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614e50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8fd2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"planets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71bab577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method              0\n",
       "number              0\n",
       "orbital_period     43\n",
       "mass              522\n",
       "distance          227\n",
       "year                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf180a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mass']= df.mass.fillna(df['mass'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7122ffa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method              0\n",
       "number              0\n",
       "orbital_period     43\n",
       "mass                0\n",
       "distance          227\n",
       "year                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d1a9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance']= df.distance.fillna(df['distance'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "719f9062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method             0\n",
       "number             0\n",
       "orbital_period    43\n",
       "mass               0\n",
       "distance           0\n",
       "year               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cadc867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajmr\\AppData\\Local\\Temp\\ipykernel_20968\\18259802.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['method'][0] = np.nan\n"
     ]
    }
   ],
   "source": [
    "df['method'][0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4577789c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>number</th>\n",
       "      <th>orbital_period</th>\n",
       "      <th>mass</th>\n",
       "      <th>distance</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>269.300000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>77.40</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>874.774000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>56.95</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>19.84</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>326.030000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>110.62</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>516.220000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>119.47</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>3.941507</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>172.00</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>2.615864</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>148.00</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>3.191524</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>174.00</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>4.125083</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>293.00</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>4.187757</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>260.00</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1035 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               method  number  orbital_period       mass  distance  year\n",
       "0                 NaN       1      269.300000   7.100000     77.40  2006\n",
       "1     Radial Velocity       1      874.774000   2.210000     56.95  2008\n",
       "2     Radial Velocity       1      763.000000   2.600000     19.84  2011\n",
       "3     Radial Velocity       1      326.030000  19.400000    110.62  2007\n",
       "4     Radial Velocity       1      516.220000  10.500000    119.47  2009\n",
       "...               ...     ...             ...        ...       ...   ...\n",
       "1030          Transit       1        3.941507   2.638161    172.00  2006\n",
       "1031          Transit       1        2.615864   2.638161    148.00  2007\n",
       "1032          Transit       1        3.191524   2.638161    174.00  2007\n",
       "1033          Transit       1        4.125083   2.638161    293.00  2008\n",
       "1034          Transit       1        4.187757   2.638161    260.00  2008\n",
       "\n",
       "[1035 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e34c0671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method             1\n",
       "number             0\n",
       "orbital_period    43\n",
       "mass               0\n",
       "distance           0\n",
       "year               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "163a1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['method']= df.method.fillna(df['method'].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dcdba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method             0\n",
       "number             0\n",
       "orbital_period    43\n",
       "mass               0\n",
       "distance           0\n",
       "year               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf5f34bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>number</th>\n",
       "      <th>orbital_period</th>\n",
       "      <th>mass</th>\n",
       "      <th>distance</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>269.300000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>77.40</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>874.774000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>56.95</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>19.84</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>326.030000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>110.62</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radial Velocity</td>\n",
       "      <td>1</td>\n",
       "      <td>516.220000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>119.47</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>3.941507</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>172.00</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>2.615864</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>148.00</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>3.191524</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>174.00</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>4.125083</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>293.00</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>Transit</td>\n",
       "      <td>1</td>\n",
       "      <td>4.187757</td>\n",
       "      <td>2.638161</td>\n",
       "      <td>260.00</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1035 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               method  number  orbital_period       mass  distance  year\n",
       "0     Radial Velocity       1      269.300000   7.100000     77.40  2006\n",
       "1     Radial Velocity       1      874.774000   2.210000     56.95  2008\n",
       "2     Radial Velocity       1      763.000000   2.600000     19.84  2011\n",
       "3     Radial Velocity       1      326.030000  19.400000    110.62  2007\n",
       "4     Radial Velocity       1      516.220000  10.500000    119.47  2009\n",
       "...               ...     ...             ...        ...       ...   ...\n",
       "1030          Transit       1        3.941507   2.638161    172.00  2006\n",
       "1031          Transit       1        2.615864   2.638161    148.00  2007\n",
       "1032          Transit       1        3.191524   2.638161    174.00  2007\n",
       "1033          Transit       1        4.125083   2.638161    293.00  2008\n",
       "1034          Transit       1        4.187757   2.638161    260.00  2008\n",
       "\n",
       "[1035 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c346eaf",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Imbalanced dataset means data has a lot of bias to a particular output\n",
    "Imbalanced data refers to a situation in a dataset where the distribution of classes or categories is highly skewed, meaning that one class has significantly more instances than the others.\n",
    "\n",
    "Its handled by resampling like upsampling and downsampleing where minority group is increased or majority group is decreased. Also SMOTE is used. Its a synthetic way of inroducing new values in minority group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d400aac",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required.\n",
    "\n",
    "\n",
    "Upsampleing means the minority group data points is increased upto majority group data points. This is preferred method\n",
    "Down sampling measn the majority group datapoints is decreased upto upto minority data points. This is not prederred as data is lost.\n",
    "\n",
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced data in machine learning. These techniques aim to balance the distribution of classes in the dataset by either increasing or decreasing the number of instances of certain classes.\n",
    "\n",
    "Up-sampling (Over-sampling):\n",
    "Up-sampling involves increasing the number of instances in the minority class to balance the dataset. This can be done by replicating existing instances in the minority class or generating synthetic instances using techniques like SMOTE (Synthetic Minority Over-sampling Technique). The goal is to ensure that the minority class has a similar number of instances as the majority class, thereby preventing the model from being biased towards the majority class.\n",
    "Example:\n",
    "Let's say we have a dataset for a rare disease prediction with two classes: \"Healthy\" (majority class) and \"Disease\" (minority class). The \"Healthy\" class has 900 instances, while the \"Disease\" class has only 100 instances. To up-sample the \"Disease\" class, we can create synthetic instances using SMOTE or simply replicate some existing instances. After up-sampling, both classes might have 900 instances each.\n",
    "\n",
    "Down-sampling (Under-sampling):\n",
    "Down-sampling involves reducing the number of instances in the majority class to balance the dataset. This can be achieved by randomly removing instances from the majority class until the distribution of classes becomes balanced. The goal is to avoid the model being biased towards the majority class and to allow the minority class to be better represented.\n",
    "Example:\n",
    "Continuing with the previous example, we can perform down-sampling on the \"Healthy\" class to match the number of instances in the \"Disease\" class. After down-sampling, both classes might have 100 instances each.\n",
    "\n",
    "When to use Up-sampling and Down-sampling:\n",
    "\n",
    "Up-sampling is useful when the minority class is underrepresented, and generating more instances of the minority class can help the model learn more about its characteristics. This is typically done when the available data for the minority class is limited.\n",
    "\n",
    "Down-sampling is useful when the majority class is dominant, and reducing its instances can help balance the dataset. This can be beneficial when the majority class data overwhelms the minority class data, leading to bias in the model.\n",
    "\n",
    "It's important to note that both up-sampling and down-sampling have their pros and cons. Up-sampling can increase the risk of overfitting, especially if synthetic instances are not generated carefully. Down-sampling can lead to a loss of valuable information from the majority class. Therefore, it's essential to carefully choose the appropriate technique based on the dataset size, the context of the problem, and the specific machine learning algorithm being used. In some cases, a combination of both techniques or other methods, like using ensemble models, may be more effective in handling imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936c31d",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new variations of the existing data. This approach is commonly used in machine learning, especially in the context of computer vision and natural language processing tasks, where having more diverse and representative data can improve the performance of models.\n",
    "\n",
    "\n",
    "One popular technique for data augmentation in imbalanced datasets, particularly for tackling the imbalanced class problem, is the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE is designed to address the issue of the minority class being underrepresented in the dataset, which can lead to biased models and poor performance on the minority class.\n",
    "\n",
    " Here's how SMOTE works:\n",
    "\n",
    "For each instance in the minority class, SMOTE selects its k nearest neighbors from the same class. The value of k is a user-defined hyperparameter.\n",
    "\n",
    "It then randomly selects one of the k neighbors and creates a synthetic instance along the line connecting the original instance and the chosen neighbor.\n",
    "\n",
    "The synthetic instance is generated by combining features from the original instance and the selected neighbor. The features are combined in a weighted manner, where the weight is a random value between 0 and 1.\n",
    "\n",
    "The process is repeated until the desired level of over-sampling is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23455c",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Outliers are the datapoints which lie below the lower fence or above the higher fence.\n",
    "Outlier are noise in the data. Those data does not belong to that group. So by removing the outliers the noise of the data is reduced theryby increasing the model perfromace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba256f6",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Use mean ,median in case of continous data and mode in case of categorical data. There is also a techinique called random where any random value is taken and null value is filled.\n",
    "\n",
    "Removing the row or the columns that have null data but this is not preffered as there is loss of data\n",
    "\n",
    "Using data augmentation techniques to create synthetic data to fill in the missing values, especially useful for image or text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d512f",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "When dealing with missing data in a large dataset, you can use several strategies to determine if the missing data is missing at random (MAR) or if there is a pattern to the missingness. Understanding the missingness pattern is crucial for choosing appropriate imputation methods and assessing potential biases in the data analysis. Here are some strategies you can employ:\n",
    "\n",
    "Visualizations:\n",
    "\n",
    "Create visualizations, such as heatmaps or bar plots, to visualize the distribution of missing values across different variables. Look for patterns or trends in the missingness, such as missing values occurring more frequently in certain variables or certain combinations of variables.\n",
    "Missing Data Summary Statistics:\n",
    "\n",
    "Compute summary statistics related to missing data, such as the percentage of missing values for each variable, and compare these statistics across different subsets of the data (e.g., different classes or groups). If the missingness differs significantly between subsets, it could indicate a pattern in the missing data.\n",
    "Missing Data Patterns Across Time:\n",
    "\n",
    "If your dataset has a time component, examine missing data patterns over time. Look for any temporal trends or patterns in the missingness.\n",
    "Missing Data Mechanism Tests:\n",
    "\n",
    "Perform statistical tests to assess whether the missing data follows a specific missing data mechanism. For example, you can use the Little's MCAR (Missing Completely at Random) test or the Missing Indicator Test to determine if the missingness is completely at random or missing not at random (MNAR).\n",
    "Imputation Performance:\n",
    "\n",
    "Compare the performance of different imputation methods. If the performance of imputation varies across different variables or subsets of the data, it could indicate a pattern in the missing data.\n",
    "Domain Knowledge:\n",
    "\n",
    "Leverage your domain knowledge to understand if there are plausible reasons for the missingness. For example, certain demographic data might be missing due to survey respondents' refusal to provide that information.\n",
    "Data Exploration and Feature Analysis:\n",
    "\n",
    "Examine relationships between variables and the missingness of other variables. Conduct exploratory data analysis to understand if the missingness is related to specific data patterns or certain values of other variables.\n",
    "Multiple Imputation and Sensitivity Analysis:\n",
    "\n",
    "Use multiple imputation to create multiple plausible datasets with imputed values, and analyze the results from different imputed datasets. Sensitivity analysis can help evaluate the robustness of conclusions across different imputation scenarios.\n",
    "By employing these strategies, you can gain insights into the missing data patterns and make informed decisions on how to handle the missing data appropriately in your analysis. Remember that there is no one-size-fits-all solution, and the choice of handling missing data should be based on the specific characteristics and context of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece020d0",
   "metadata": {},
   "source": [
    "\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "When working with imbalanced medical diagnosis datasets, where the majority of patients do not have the condition of interest (negative class) and a small percentage do have the condition (positive class), standard performance metrics like accuracy can be misleading. This is because a model that simply predicts the majority class for all instances can achieve high accuracy due to the class imbalance, while failing to detect the positive class correctly. To address this issue and get a more reliable evaluation of the model's performance, you can use several strategies:\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "Analyze the confusion matrix, which provides a breakdown of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). From the confusion matrix, you can calculate other performance metrics.\n",
    "Precision, Recall, and F1-Score:\n",
    "\n",
    "Use precision (also known as positive predictive value) and recall (also known as sensitivity or true positive rate) to evaluate the model's performance. Precision measures the ability of the model to correctly identify positive instances among all predicted positive instances. Recall measures the ability of the model to correctly identify positive instances among all actual positive instances. F1-score is the harmonic mean of precision and recall and provides a balanced evaluation metric.\n",
    "Area Under the Receiver Operating Characteristic curve (AUC-ROC):\n",
    "\n",
    "ROC curve plots the true positive rate (recall) against the false positive rate (1-specificity) for different classification thresholds. AUC-ROC summarizes the model's performance across all classification thresholds and is particularly useful when the model's output probabilities can vary.\n",
    "Area Under the Precision-Recall curve (AUC-PR):\n",
    "\n",
    "PR curve plots precision against recall for different classification thresholds. AUC-PR provides a good evaluation metric when dealing with imbalanced datasets.\n",
    "Stratified Cross-Validation:\n",
    "\n",
    "Use stratified cross-validation to ensure that the class distribution is maintained in each fold. This helps in obtaining more robust and representative evaluation results.\n",
    "Class Weights or Sampling Methods:\n",
    "\n",
    "In some machine learning algorithms, you can assign different class weights to give more importance to the minority class. Alternatively, you can use sampling techniques like oversampling the minority class or undersampling the majority class to balance the class distribution during training.\n",
    "Ensemble Methods:\n",
    "\n",
    "Consider using ensemble methods like Random Forest or Gradient Boosting, which can handle imbalanced data more effectively than individual models.\n",
    "Remember that the choice of performance metrics and evaluation strategies should align with the specific goals and requirements of the medical diagnosis project. It's also essential to interpret the results in the context of the domain and consider the consequences of false positives and false negatives in medical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495dbab2",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "Random Under-sampling:\n",
    "\n",
    "Randomly select a subset of instances from the majority class to match the number of instances in the minority class. This is a straightforward method, but it may lead to loss of information.\n",
    "\n",
    "or SMOTE can be used for downsampleing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811072d",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "When dealing with a dataset that is unbalanced and contains a rare event (i.e., a minority class with a low percentage of occurrences), you can employ up-sampling techniques to increase the number of instances in the minority class. Up-sampling aims to balance the class distribution by generating synthetic instances of the minority class. Here are some methods you can use to up-sample the minority class:\n",
    "\n",
    "Random Over-sampling:\n",
    "\n",
    "Randomly duplicate instances from the minority class to increase its size. This is a simple method but may lead to overfitting if the same instances are repeated too many times.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "As mentioned earlier, SMOTE generates synthetic instances by interpolating between existing minority class instances and their k-nearest neighbors. This method is effective in creating diverse synthetic samples and reducing overfitting.\n",
    "ADASYN (Adaptive Synthetic Sampling):\n",
    "\n",
    "ADASYN is an extension of SMOTE that generates more synthetic instances for difficult-to-learn examples, making it particularly useful when the decision boundary between classes is complex.\n",
    "Borderline-SMOTE:\n",
    "\n",
    "Borderline-SMOTE focuses on generating synthetic instances near the decision boundary, which helps improve classification performance for ambiguous instances.\n",
    "ROSE (Random Over-Sampling Examples):\n",
    "\n",
    "ROSE creates synthetic instances by fitting a smoothed bootstrap density estimator to the minority class, which reduces the risk of overfitting.\n",
    "Synthetic Data Augmentation:\n",
    "\n",
    "For image or text data, you can use data augmentation techniques to create synthetic data points with slight variations to the original minority class samples.\n",
    "Ensemble Methods:\n",
    "\n",
    "Use ensemble methods like EasyEnsemble or BalanceCascade, which create multiple balanced datasets by randomly selecting subsets of the majority class and combining them with the entire minority class. These datasets are then used to train different models, and their outputs are combined for final predictions.\n",
    "It's essential to be cautious when using up-sampling techniques, as generating synthetic instances should be done thoughtfully to avoid overfitting and creating unrealistic data. When up-sampling the minority class, consider combining it with down-sampling the majority class or using other methods like cost-sensitive learning to achieve a balanced and more representative dataset. As always, the choice of the method should be based on the specific characteristics of your dataset and the requirements of your rare event estimation project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace4b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
