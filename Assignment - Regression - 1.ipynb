{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91889016",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fb63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ea133",
   "metadata": {},
   "source": [
    "Simple linear regression is where you have only one independent and one dependent feature. eg predict cost of rice based on weight if rice\n",
    "\n",
    "Multiple linear regression is where yiu have many independent features and only one dependent feature. Eg predict cost of rice based of weight, variety, location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a57c80",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "Linear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It is based on several assumptions that need to hold for the results and interpretations of the regression analysis to be valid. Here are the key assumptions of linear regression:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables is assumed to be linear. This means that the change in the dependent variable is proportional to the change in the independent variables.\n",
    "\n",
    "Independence of Residuals: The residuals (the differences between the actual and predicted values) should be independent of each other. In other words, there should be no pattern or correlation in the residuals.\n",
    "\n",
    "Homoscedasticity: Also known as constant variance, this assumption states that the variability of the residuals should be roughly constant across all levels of the independent variables. This implies that the spread of residuals should be roughly the same across the range of predicted values.\n",
    "\n",
    "Normality of Residuals: The residuals should follow a normal distribution. This assumption is important for making statistical inferences and constructing confidence intervals and hypothesis tests.\n",
    "\n",
    "No or Little Multicollinearity: If there are multiple independent variables in the model, they should not be highly correlated with each other. High multicollinearity can lead to unstable coefficient estimates and difficulty in interpreting the individual effects of the variables.\n",
    "\n",
    "No Endogeneity: The independent variables should be exogenous, meaning they are not influenced by the error term. If there is endogeneity (bidirectional causality), it can lead to biased coefficient estimates.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can use various diagnostic techniques:\n",
    "\n",
    "Residual Analysis: Plot the residuals against the predicted values. Look for any patterns or trends in the residuals that might violate the assumptions of linearity, independence, or homoscedasticity.\n",
    "\n",
    "Normality Tests: Plot a histogram or a Q-Q plot of the residuals to assess their normality. You can also use formal statistical tests like the Shapiro-Wilk test or the Anderson-Darling test.\n",
    "\n",
    "Homoscedasticity Tests: Create a scatter plot of residuals against predicted values. If the spread of residuals is consistent across different levels of predicted values, homoscedasticity is likely met. You can also use statistical tests like the Breusch-Pagan test or the White test.\n",
    "\n",
    "Multicollinearity Assessment: Calculate correlation coefficients between independent variables. Alternatively, use variance inflation factors (VIF) to quantify the extent of multicollinearity. High correlation coefficients or VIF values above 10 might indicate an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5eb0c5",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "\n",
    "Intercept the value of y when x = 0\n",
    "\n",
    "slope is the change in the dependent value for one unit chnage in the independent variable.\n",
    "\n",
    "example. Minimum wage of salary as the intercept and slope is the chnage in the salary package based on the experience.\n",
    "\n",
    "salary = minimum wage + slope * experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa24ec5",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Gradient descent is a technigue to find the global minima. Inother words to find the best slope for linear regression so that we can get the line of the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd7dc3",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Multiple linear regression is where yiu have many independent features and only one dependent feature. Eg predict cost of rice based of weight, variety, location\n",
    "\n",
    "Simple linear regression is where you have only one independent and one dependent feature. eg predict cost of rice based on weight if rice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa11d1",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other. \n",
    "\n",
    "Remove One Variable: If two or more variables are highly correlated, consider removing one of them from the model. Choose the variable that is less theoretically relevant or has a weaker relationship with the dependent variable.\n",
    "\n",
    "Combine Variables: Sometimes, creating a composite variable from correlated variables can help mitigate multicollinearity. For instance, if you have two variables measuring similar concepts, you could create an average or weighted average of these variables.\n",
    "\n",
    "Domain Knowledge: Rely on your understanding of the subject matter. If the correlation between variables is theoretically justified and expected, multicollinearity might not be a concern.\n",
    "\n",
    "Regularization Techniques: Techniques like Ridge Regression and Lasso Regression introduce a penalty for large coefficients, which can help stabilize coefficient estimates and reduce the impact of multicollinearity.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can transform correlated variables into uncorrelated principal components, which can then be used in the regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad239c",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Polynomial regression is when there is no linear corrleation between the dependent and independent features.\n",
    "\n",
    "In Linear regression there is a linear correlation ie positive or negative correlation between the dependent and independent features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b0763",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Polynomial Regression is an extension of linear regression that allows for modeling relationships between variables using higher-degree polynomial functions. While both linear and polynomial regression have their own strengths and weaknesses, the choice between them depends on the characteristics of the data and the underlying relationships being modeled.\n",
    "\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: Polynomial regression can capture more complex and nonlinear relationships between variables. Linear regression is limited to straight-line relationships, while polynomial regression can handle curves and bends.\n",
    "\n",
    "Better Fit: When the data exhibits curvature or nonlinear patterns, polynomial regression can provide a better fit than linear regression. It can closely match the data points and reduce the residual errors.\n",
    "\n",
    "Improved Interpretation: Polynomial regression can still provide interpretable results, especially when you're dealing with lower-degree polynomials (e.g., quadratic or cubic). These relationships might have real-world significance.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: As the degree of the polynomial increases, the model becomes more complex and prone to overfitting. High-degree polynomials can fit noise in the data and perform poorly on new, unseen data.\n",
    "\n",
    "Instability: Polynomial models with high-degree terms can exhibit instability in their coefficient estimates, leading to unreliable results and difficult interpretation.\n",
    "\n",
    "Extrapolation Issues: Polynomial regression can behave unpredictably outside the range of the data used for modeling. Extrapolation beyond the range of the data can lead to inaccurate predictions.\n",
    "\n",
    "Loss of Interpretability: With higher-degree polynomials, the model can become less interpretable as the relationships between variables become more complex.\n",
    "\n",
    "Situations for Using Polynomial Regression:\n",
    "\n",
    "Curved Relationships: When you observe that the relationship between the independent and dependent variables is not linear but has a curve or bend, polynomial regression can capture these patterns more accurately.\n",
    "\n",
    "Limited Nonlinearity: If you suspect there might be a slight curvature in the relationship but don't want to jump to a high-degree polynomial, you can use quadratic (degree 2) or cubic (degree 3) terms to allow for more flexibility without making the model too complex.\n",
    "\n",
    "Insufficient Data for Complex Models: If you have relatively limited data, using a high-degree polynomial might lead to overfitting. In such cases, it's wise to use a simpler model or consider regularization techniques.\n",
    "\n",
    "Exploratory Analysis: Polynomial regression can be used as an exploratory tool to identify the nature of the relationship between variables. If a polynomial term significantly improves the fit of the model and aligns with your domain knowledge, it might be a good choice.\n",
    "\n",
    "In summary, polynomial regression offers greater flexibility in capturing nonlinear relationships, but it comes with the trade-off of potential overfitting and complexity. It's important to carefully assess the characteristics of your data, consider the degree of polynomial, and possibly experiment with simpler linear models and regularization techniques before deciding to use polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165df3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54666b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
