{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b527aa",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Its a normalization technique. here all the values of the feature will be converted between zero and one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a48dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = sns.load_dataset('tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff9bf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "df1 = pd.DataFrame(min_max.fit_transform(df[['total_bill','tip']])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb87b88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.291579</td>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.152283</td>\n",
       "      <td>0.073333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375786</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.431713</td>\n",
       "      <td>0.256667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450775</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.543779</td>\n",
       "      <td>0.546667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.505027</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.410557</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.308965</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.329074</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "0    0.291579  0.001111\n",
       "1    0.152283  0.073333\n",
       "2    0.375786  0.277778\n",
       "3    0.431713  0.256667\n",
       "4    0.450775  0.290000\n",
       "..        ...       ...\n",
       "239  0.543779  0.546667\n",
       "240  0.505027  0.111111\n",
       "241  0.410557  0.111111\n",
       "242  0.308965  0.083333\n",
       "243  0.329074  0.222222\n",
       "\n",
       "[244 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6ad8151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1085edc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65779b9f",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Unit Vector Technique: Scales each feature value to have a magnitude of 1 while preserving its direction. The feature values are placed on the surface of a unit sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f382e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ba16758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99823771, 0.05934197],\n",
       "       [0.98735707, 0.15851187],\n",
       "       [0.98640661, 0.16432285],\n",
       "       [0.99037159, 0.13843454],\n",
       "       [0.98939488, 0.14525073],\n",
       "       [0.98309589, 0.18309141],\n",
       "       [0.97496878, 0.2223418 ],\n",
       "       [0.99333102, 0.11529735],\n",
       "       [0.99161511, 0.12922644],\n",
       "       [0.97694312, 0.21349975],\n",
       "       [0.98641987, 0.16424323],\n",
       "       [0.99009498, 0.14039917],\n",
       "       [0.99485672, 0.10129216],\n",
       "       [0.98700924, 0.16066347],\n",
       "       [0.97988851, 0.19954574],\n",
       "       [0.98389908, 0.17872495],\n",
       "       [0.9871829 , 0.15959298],\n",
       "       [0.9750328 , 0.22206088],\n",
       "       [0.97938658, 0.20199487],\n",
       "       [0.98709527, 0.1601341 ],\n",
       "       [0.97504726, 0.22199737],\n",
       "       [0.9909398 , 0.13430677],\n",
       "       [0.99014941, 0.14001479],\n",
       "       [0.98201   , 0.18882891],\n",
       "       [0.98737215, 0.15841793],\n",
       "       [0.99147891, 0.1302673 ],\n",
       "       [0.98899596, 0.14794255],\n",
       "       [0.98780711, 0.15568276],\n",
       "       [0.98092686, 0.19437721],\n",
       "       [0.98854552, 0.15092298],\n",
       "       [0.98866899, 0.15011205],\n",
       "       [0.99084659, 0.13499272],\n",
       "       [0.98073067, 0.19536467],\n",
       "       [0.99306186, 0.11759312],\n",
       "       [0.98350502, 0.18088084],\n",
       "       [0.98899056, 0.14797864],\n",
       "       [0.9925654 , 0.1217125 ],\n",
       "       [0.98395349, 0.17842512],\n",
       "       [0.99244848, 0.12266217],\n",
       "       [0.98745639, 0.15789197],\n",
       "       [0.99038917, 0.13830871],\n",
       "       [0.9895835 , 0.14396003],\n",
       "       [0.97674434, 0.21440729],\n",
       "       [0.99083017, 0.1351132 ],\n",
       "       [0.98345319, 0.18116243],\n",
       "       [0.98681354, 0.16186116],\n",
       "       [0.9756262 , 0.21943909],\n",
       "       [0.983282  , 0.18208926],\n",
       "       [0.99743203, 0.07161946],\n",
       "       [0.98645298, 0.16404429],\n",
       "       [0.98070081, 0.19551452],\n",
       "       [0.96952977, 0.24497351],\n",
       "       [0.98902579, 0.14774301],\n",
       "       [0.98790759, 0.15504385],\n",
       "       [0.98588897, 0.16740055],\n",
       "       [0.98416747, 0.17724104],\n",
       "       [0.99689977, 0.07868191],\n",
       "       [0.99839096, 0.05670528],\n",
       "       [0.98796171, 0.15469863],\n",
       "       [0.99041991, 0.13808838],\n",
       "       [0.98771556, 0.15626254],\n",
       "       [0.98967534, 0.14332735],\n",
       "       [0.98423933, 0.17684155],\n",
       "       [0.97951611, 0.20136581],\n",
       "       [0.98892398, 0.14842293],\n",
       "       [0.98791805, 0.15497718],\n",
       "       [0.98891429, 0.14848744],\n",
       "       [0.95082902, 0.30971629],\n",
       "       [0.9951003 , 0.09887057],\n",
       "       [0.99044476, 0.13791003],\n",
       "       [0.98683411, 0.16173571],\n",
       "       [0.9849053 , 0.17309408],\n",
       "       [0.99323616, 0.11611175],\n",
       "       [0.98099635, 0.19402618],\n",
       "       [0.98902973, 0.14771659],\n",
       "       [0.99300147, 0.11810198],\n",
       "       [0.98554889, 0.16939122],\n",
       "       [0.98935914, 0.14549399],\n",
       "       [0.99142462, 0.13067987],\n",
       "       [0.98793839, 0.15484749],\n",
       "       [0.98830106, 0.1525156 ],\n",
       "       [0.97980406, 0.19996001],\n",
       "       [0.98388554, 0.17879946],\n",
       "       [0.98849725, 0.15123887],\n",
       "       [0.99202757, 0.12602102],\n",
       "       [0.98916225, 0.14682655],\n",
       "       [0.98842426, 0.15171516],\n",
       "       [0.97688607, 0.21376063],\n",
       "       [0.9731012 , 0.23037807],\n",
       "       [0.99009867, 0.14037316],\n",
       "       [0.99468088, 0.10300458],\n",
       "       [0.98810609, 0.15377374],\n",
       "       [0.98521175, 0.17134117],\n",
       "       [0.96699774, 0.25478494],\n",
       "       [0.98994949, 0.14142136],\n",
       "       [0.99313879, 0.11694166],\n",
       "       [0.98942047, 0.14507631],\n",
       "       [0.9923159 , 0.12373016],\n",
       "       [0.98995892, 0.14135539],\n",
       "       [0.99283152, 0.11952225],\n",
       "       [0.97659027, 0.21510799],\n",
       "       [0.98150229, 0.19145038],\n",
       "       [0.99841143, 0.05634376],\n",
       "       [0.98816699, 0.15338185],\n",
       "       [0.9815078 , 0.19142217],\n",
       "       [0.99434827, 0.10616739],\n",
       "       [0.98092896, 0.1943666 ],\n",
       "       [0.98582805, 0.16775892],\n",
       "       [0.97940711, 0.20189532],\n",
       "       [0.96308275, 0.26920552],\n",
       "       [0.97780241, 0.20952909],\n",
       "       [0.99062113, 0.1366374 ],\n",
       "       [0.99452547, 0.1044944 ],\n",
       "       [0.99437962, 0.1058734 ],\n",
       "       [0.98811258, 0.15373202],\n",
       "       [0.9801647 , 0.19818466],\n",
       "       [0.98595419, 0.16701596],\n",
       "       [0.99022651, 0.13946852],\n",
       "       [0.98967697, 0.14331605],\n",
       "       [0.99272781, 0.12038062],\n",
       "       [0.98102995, 0.19385622],\n",
       "       [0.99225511, 0.12421674],\n",
       "       [0.98497764, 0.17268191],\n",
       "       [0.99222995, 0.12441755],\n",
       "       [0.98021649, 0.19792833],\n",
       "       [0.99021357, 0.1395603 ],\n",
       "       [0.98524568, 0.17114596],\n",
       "       [0.99064659, 0.1364527 ],\n",
       "       [0.9849053 , 0.17309408],\n",
       "       [0.99546798, 0.09509729],\n",
       "       [0.99692399, 0.07837453],\n",
       "       [0.99039401, 0.13827405],\n",
       "       [0.99110348, 0.13309357],\n",
       "       [0.98695377, 0.16100388],\n",
       "       [0.98452739, 0.17523078],\n",
       "       [0.98938373, 0.14532664],\n",
       "       [0.98176841, 0.19008101],\n",
       "       [0.99015833, 0.13995171],\n",
       "       [0.99227788, 0.12403473],\n",
       "       [0.97885648, 0.20454828],\n",
       "       [0.98051586, 0.19643993],\n",
       "       [0.98145122, 0.19171205],\n",
       "       [0.99271283, 0.12050411],\n",
       "       [0.98334222, 0.18176381],\n",
       "       [0.9903434 , 0.13863602],\n",
       "       [0.98424492, 0.17681046],\n",
       "       [0.9973489 , 0.07276795],\n",
       "       [0.99070276, 0.13604427],\n",
       "       [0.98471252, 0.17418739],\n",
       "       [0.96632031, 0.25734229],\n",
       "       [0.98457858, 0.17494289],\n",
       "       [0.98859692, 0.15058597],\n",
       "       [0.98763272, 0.15678526],\n",
       "       [0.99669804, 0.0811974 ],\n",
       "       [0.99492193, 0.10064966],\n",
       "       [0.98549631, 0.16969685],\n",
       "       [0.99465602, 0.10324435],\n",
       "       [0.98893635, 0.14834045],\n",
       "       [0.98152754, 0.1913209 ],\n",
       "       [0.99272506, 0.12040328],\n",
       "       [0.98700727, 0.1606756 ],\n",
       "       [0.98105465, 0.19373117],\n",
       "       [0.99247442, 0.12245212],\n",
       "       [0.98967534, 0.14332735],\n",
       "       [0.98563832, 0.16887007],\n",
       "       [0.99007829, 0.14051682],\n",
       "       [0.99422916, 0.10727713],\n",
       "       [0.9900802 , 0.14050334],\n",
       "       [0.98863996, 0.15030315],\n",
       "       [0.98275687, 0.18490251],\n",
       "       [0.98117767, 0.1931072 ],\n",
       "       [0.98060452, 0.19599686],\n",
       "       [0.81525026, 0.57910881],\n",
       "       [0.99505264, 0.09934906],\n",
       "       [0.97286824, 0.23135987],\n",
       "       [0.99556187, 0.09410934],\n",
       "       [0.99380899, 0.11110218],\n",
       "       [0.99059557, 0.13682259],\n",
       "       [0.92307692, 0.38461538],\n",
       "       [0.99478667, 0.10197784],\n",
       "       [0.99440752, 0.10561096],\n",
       "       [0.971905  , 0.23537348],\n",
       "       [0.99703505, 0.07694868],\n",
       "       [0.96283011, 0.27010771],\n",
       "       [0.99727446, 0.07378109],\n",
       "       [0.97201936, 0.23490076],\n",
       "       [0.98626609, 0.16516418],\n",
       "       [0.99785133, 0.0655188 ],\n",
       "       [0.98190985, 0.18934901],\n",
       "       [0.98533672, 0.17062108],\n",
       "       [0.99546119, 0.09516837],\n",
       "       [0.97835547, 0.20693132],\n",
       "       [0.99597319, 0.08965159],\n",
       "       [0.99159327, 0.12939395],\n",
       "       [0.97210987, 0.2345259 ],\n",
       "       [0.98233857, 0.18711211],\n",
       "       [0.9818027 , 0.18990381],\n",
       "       [0.99334115, 0.11521006],\n",
       "       [0.9883717 , 0.15205718],\n",
       "       [0.98921918, 0.14644251],\n",
       "       [0.97790174, 0.20906504],\n",
       "       [0.98778182, 0.15584313],\n",
       "       [0.9883717 , 0.15205718],\n",
       "       [0.98857982, 0.15069814],\n",
       "       [0.98154316, 0.19124075],\n",
       "       [0.98130713, 0.19244821],\n",
       "       [0.99187684, 0.12720196],\n",
       "       [0.99701346, 0.077228  ],\n",
       "       [0.99652023, 0.0833513 ],\n",
       "       [0.98506977, 0.17215561],\n",
       "       [0.99779396, 0.06638682],\n",
       "       [0.98071158, 0.19546048],\n",
       "       [0.98309941, 0.18307252],\n",
       "       [0.98271253, 0.18513801],\n",
       "       [0.97439703, 0.22483424],\n",
       "       [0.99638411, 0.08496299],\n",
       "       [0.99436913, 0.10597184],\n",
       "       [0.99172875, 0.12835143],\n",
       "       [0.98313005, 0.18290792],\n",
       "       [0.99478573, 0.10198699],\n",
       "       [0.98402491, 0.17803082],\n",
       "       [0.96798392, 0.25101222],\n",
       "       [0.97586485, 0.21837535],\n",
       "       [0.98283039, 0.18451134],\n",
       "       [0.9931405 , 0.11692712],\n",
       "       [0.98839977, 0.15187458],\n",
       "       [0.98091575, 0.19443325],\n",
       "       [0.98941028, 0.14514576],\n",
       "       [0.97966223, 0.20065371],\n",
       "       [0.99163038, 0.1291092 ],\n",
       "       [0.99654862, 0.08301113],\n",
       "       [0.98220682, 0.18780245],\n",
       "       [0.95991662, 0.28028573],\n",
       "       [0.99081337, 0.13523636],\n",
       "       [0.98184827, 0.18966805],\n",
       "       [0.99238364, 0.12318566],\n",
       "       [0.99686539, 0.0791163 ],\n",
       "       [0.99936557, 0.03561553],\n",
       "       [0.99161275, 0.12924453],\n",
       "       [0.97983374, 0.19981453],\n",
       "       [0.99730368, 0.07338511],\n",
       "       [0.99613098, 0.08788099],\n",
       "       [0.99521256, 0.09773412],\n",
       "       [0.98747998, 0.15774441]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(df[['total_bill','tip']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "PCA is used to convert 2d feture or nd feature to 1d feature with a minimal loss of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "799d6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[160  55]\n",
      " [170  62]\n",
      " [155  50]\n",
      " [175  70]\n",
      " [168  60]]\n",
      "Data after Standardization:\n",
      " [[-0.78079594 -0.65272991]\n",
      " [ 0.61348253  0.38570404]\n",
      " [-1.47793517 -1.39446845]\n",
      " [ 1.31062176  1.5724857 ]\n",
      " [ 0.33462683  0.08900862]]\n",
      "Principal Component (Top):\n",
      " [-0.70710678  0.70710678]\n",
      "Data in Reduced Dimension (1D):\n",
      " [ 0.09055636 -0.16106371  0.05901989  0.18516577 -0.1736783 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data with two features: Height and Weight\n",
    "data = np.array([[160, 55],\n",
    "                 [170, 62],\n",
    "                 [155, 50],\n",
    "                 [175, 70],\n",
    "                 [168, 60]])\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data_standardized = (data - mean) / std\n",
    "\n",
    "# Step 2 and 3: Calculate Covariance Matrix and Compute Eigenvectors and Eigenvalues\n",
    "cov_matrix = np.cov(data_standardized.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Step 4: Select Principal Components (eigenvectors with highest eigenvalues)\n",
    "# In this case, we will choose the first principal component (the most important one)\n",
    "top_principal_component = eigenvectors[:, 0]\n",
    "\n",
    "# Step 5: Project Data onto New Feature Space (1D)\n",
    "data_reduced = data_standardized.dot(top_principal_component)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Data after Standardization:\\n\", data_standardized)\n",
    "print(\"Principal Component (Top):\\n\", top_principal_component)\n",
    "print(\"Data in Reduced Dimension (1D):\\n\", data_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184585e7",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "PCA (Principal Component Analysis) is a technique that can be used for both dimensionality reduction and feature extraction. While dimensionality reduction focuses on reducing the number of dimensions (features) in a dataset, feature extraction aims to create new, more informative features from the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7eec42",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique that can be used for both dimensionality reduction and feature extraction. While dimensionality reduction focuses on reducing the number of dimensions (features) in a dataset, feature extraction aims to create new, more informative features from the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a33eb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[160  55]\n",
      " [170  62]\n",
      " [155  50]\n",
      " [175  70]\n",
      " [168  60]]\n",
      "Data after Standardization:\n",
      " [[-0.78079594 -0.65272991]\n",
      " [ 0.61348253  0.38570404]\n",
      " [-1.47793517 -1.39446845]\n",
      " [ 1.31062176  1.5724857 ]\n",
      " [ 0.33462683  0.08900862]]\n",
      "Top Principal Component:\n",
      " [[0.70710678 0.70710678]]\n",
      "Data in Feature-Extracted Space (1D):\n",
      " [[-1.01365585]\n",
      " [ 0.7065316 ]\n",
      " [-2.03109608]\n",
      " [ 2.03866483]\n",
      " [ 0.2995555 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data with two features: Height and Weight\n",
    "data = np.array([[160, 55],\n",
    "                 [170, 62],\n",
    "                 [155, 50],\n",
    "                 [175, 70],\n",
    "                 [168, 60]])\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data_standardized = (data - mean) / std\n",
    "\n",
    "# Step 2 and 3: Compute Principal Components\n",
    "pca = PCA()\n",
    "pca.fit(data_standardized)\n",
    "\n",
    "# Step 4: Select Top Principal Components (Let's choose 1 component in this example)\n",
    "k = 1\n",
    "top_principal_components = pca.components_[:k]\n",
    "\n",
    "# Step 5: Project Data onto New Feature Space (1D)\n",
    "data_feature_extracted = data_standardized.dot(top_principal_components.T)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Data after Standardization:\\n\", data_standardized)\n",
    "print(\"Top Principal Component:\\n\", top_principal_components)\n",
    "print(\"Data in Feature-Extracted Space (1D):\\n\", data_feature_extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c01437",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6679f871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[15.   4.5 30. ]\n",
      " [20.   4.  25. ]\n",
      " [12.   3.8 40. ]\n",
      " [25.   4.7 35. ]]\n",
      "Scaled Data (Min-Max Scaling):\n",
      " [[0.23076923 0.77777778 0.33333333]\n",
      " [0.61538462 0.22222222 0.        ]\n",
      " [0.         0.         1.        ]\n",
      " [1.         1.         0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([[15, 4.5, 30],\n",
    "                 [20, 4.0, 25],\n",
    "                 [12, 3.8, 40],\n",
    "                 [25, 4.7, 35]])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data (Min-Max Scaling):\\n\", scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b91149",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices with a dataset that contains many features, PCA (Principal Component Analysis) can be a valuable technique for reducing the dimensionality of the dataset. By using PCA, we can extract the most relevant information from the original features and transform them into a smaller set of uncorrelated principal components. This can help simplify the model, improve computational efficiency, and potentially enhance prediction accuracy.\n",
    "\n",
    "Here's how you can use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "Data Preprocessing:\n",
    "Ensure that the data is properly preprocessed, handling any missing values and outliers. Additionally, it's essential to standardize the numerical features so that they have zero mean and unit variance. Standardization is crucial for PCA to treat all features equally and prevent those with larger scales from dominating the analysis.\n",
    "\n",
    "Calculate Covariance Matrix:\n",
    "Compute the covariance matrix of the standardized data. The covariance matrix represents the relationships and variances between different features.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues:\n",
    "Compute the eigenvectors and eigenvalues from the covariance matrix. The eigenvectors represent the directions (principal components) along which the data varies the most, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Select Principal Components:\n",
    "Determine the number of principal components (k) you want to retain. You can do this by looking at the explained variance ratio, which represents the proportion of variance explained by each principal component. Choose a value of k that captures a significant amount of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "Project Data onto New Feature Space:\n",
    "Project the original data onto the new feature space defined by the selected k principal components. This projection will produce a new dataset with reduced dimensionality, consisting of k uncorrelated features.\n",
    "\n",
    "Optional: Model Building and Evaluation:\n",
    "Use the reduced dataset with k principal components to train your prediction model. Evaluate the model's performance and compare it to the performance when using the full set of features. Note that while PCA can improve computational efficiency and help with overfitting, it may not always result in better prediction accuracy, especially if crucial information is lost in the dimensionality reduction.\n",
    "\n",
    "By following these steps, you can effectively use PCA to reduce the dimensionality of the dataset for predicting stock prices, transforming the original high-dimensional dataset into a smaller set of meaningful and uncorrelated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bddb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a13efc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [15]\n",
      " [20]]\n",
      "Scaled Data (Min-Max Scaling):\n",
      " [[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([[1,5,10,15,20]])\n",
    "data = data.reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data (Min-Max Scaling):\\n\", scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415035f",
   "metadata": {},
   "source": [
    "When performing feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain is a critical decision. The goal is to strike a balance between reducing dimensionality and retaining enough variance to capture the essential information.\n",
    "\n",
    "To decide how many principal components to retain, you can consider the following approaches:\n",
    "\n",
    "Explained Variance:\n",
    "Calculate the explained variance ratio for each principal component. The explained variance ratio indicates the proportion of the total variance in the data explained by each principal component. Choose the number of principal components that collectively explain a significant portion of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "Scree Plot:\n",
    "Plot the eigenvalues (variances) of the principal components in descending order. The scree plot helps visualize the proportion of variance explained by each principal component. Choose the \"elbow point\" on the scree plot, which is the point where the eigenvalues start to level off, indicating that additional principal components add less variance.\n",
    "\n",
    "Domain Knowledge:\n",
    "Consider the nature of the problem and the importance of specific features in the domain. If some features are known to be more critical than others, you may choose to retain fewer principal components and discard less informative features.\n",
    "\n",
    "Let's assume you have calculated the explained variance ratio for the given dataset and obtained the following results:\n",
    "\n",
    "Principal Component\tExplained Variance Ratio\n",
    "PC1\t0.70\n",
    "PC2\t0.20\n",
    "PC3\t0.07\n",
    "PC4\t0.02\n",
    "PC5\t0.01\n",
    "Based on the explained variance ratios, the cumulative variance explained by the first two principal components (PC1 and PC2) is 0.70 + 0.20 = 0.90. This means that these two principal components capture 90% of the total variance in the data. You may decide to retain these two principal components as they explain a significant portion of the information in the original features.\n",
    "\n",
    "Ultimately, the choice of the number of principal components to retain depends on the specific requirements of your project, the amount of variance you want to preserve, and the trade-off between dimensionality reduction and information loss. In this case, retaining the first two principal components (PC1 and PC2) seems like a reasonable choice as they collectively capture a high proportion of the data's variance and can effectively represent the essential information in the original features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
