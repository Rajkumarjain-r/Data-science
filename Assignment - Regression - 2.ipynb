{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a872e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8e4a7",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "R Square is the metric to find the performance of the regression model. Basically mean of y is calucated and a line is drawn and the sum of sqaure of erros is calculated. Also the sum of square of error of data points with the linr of best fit is also calculated.\n",
    "\n",
    "The formula is \n",
    "\n",
    "1 - Sum of square of errors with the line of best fit / sum of square of error with the mean line of y\n",
    "\n",
    "1 - SSR/SST\n",
    " \n",
    "SSR sum of sqare of errors, SST sum of sqaure of totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Adjusted R2 is a better metric for accuracy and its immute of new features if added. but I r2 will increase more or decrease more if new feature is been added.\n",
    "\n",
    "the formula is \n",
    "\n",
    "1 - (1-R2)*(N-1)/(N-P-1)\n",
    "\n",
    "R2 is r square metric\n",
    "N is no of datapoints\n",
    "P is no of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574efcb",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "\n",
    "When you have more features or multiple linear regression or new feature does not contribute much to output.\n",
    "\n",
    "Adjusted R-squared, on the other hand, penalizes the inclusion of unnecessary variables by adjusting for the number of predictors in the model. It provides a more reliable assessment of the goodness of fit of a regression model when comparing models with different numbers of predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf239ba",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "MSE is mean square error. MSE = (1/n) * Σ(yᵢ - ŷᵢ)² \n",
    "\n",
    "Yi is the o/p and yi^ is the predicted op from the model. Basically its to find the error in the model usinf a quadratic equtaion\n",
    "\n",
    "It has one global and one local minima and its differntiable\n",
    "the disadv is its not rebust to outliers and since we a squaring its not in the same unit\n",
    "\n",
    "MAE is mean absolute error. MAE = (1/n) * Σ|yᵢ - ŷᵢ|\n",
    "\n",
    "Yi is the o/p and yi^ is the predicted op from the model. Basically its to find the error in the model usinf a absolute value\n",
    "\n",
    "the adv is that its robust to outliers and it will be in the same unit.\n",
    "\n",
    "\n",
    "RMSE - is the root of mean square error. MSE**.5\n",
    "\n",
    "its a metric to find the error in the model.\n",
    "\n",
    "Its has the same unit and differentiable but is not robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d14407",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "MSE is mean square error. MSE = (1/n) * Σ(yᵢ - ŷᵢ)² \n",
    "\n",
    "Yi is the o/p and yi^ is the predicted op from the model. Basically its to find the error in the model usinf a quadratic equtaion\n",
    "\n",
    "It has one global and one local minima and its differntiable\n",
    "the disadv is its not rebust to outliers and since we a squaring its not in the same unit\n",
    "\n",
    "MAE is mean absolute error. MAE = (1/n) * Σ|yᵢ - ŷᵢ|\n",
    "\n",
    "Yi is the o/p and yi^ is the predicted op from the model. Basically its to find the error in the model usinf a absolute value\n",
    "\n",
    "the adv is that its robust to outliers and it will be in the same unit.\n",
    "\n",
    "\n",
    "RMSE - is the root of mean square error. MSE**.5\n",
    "\n",
    "its a metric to find the error in the model.\n",
    "\n",
    "Its has the same unit and differentiable but is not robust to outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Lasso Regression is L1 regularization and its to feature engineering to remove the features that dont contribute to the output.\n",
    "\n",
    "Where as in Ridge regression its used to penalize the cost function so that the over fitting is reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689851c",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "We can use Ridge regression if the model is overfitted. Ridge regresssion penalises the cost function by introducing lambda that is derived from the hyper parameter tuning\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)  # alpha is the regularization strength\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_predictions = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941f78e",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Regularized linear models, such as Lasso and Ridge regression, are powerful tools for addressing overfitting and improving the generalization of linear regression models. However, like any technique, they have limitations and situations where they may not be the best choice for regression analysis:\n",
    "\n",
    "Loss of Interpretability: Regularization methods can lead to some coefficients being shrunk towards zero, effectively excluding those features from the model. While this can be beneficial for simplifying the model and reducing overfitting, it also means that the model might not provide insights into the individual contributions of these excluded features.\n",
    "\n",
    "Feature Scaling: Regularization techniques are sensitive to the scale of features. If your features are not scaled properly, their impact on the regularization term can be uneven, potentially leading to suboptimal results. Proper feature scaling is required to ensure the regularization works as intended.\n",
    "\n",
    "High-Dimensional Data: While regularization is particularly useful when dealing with high-dimensional data, it might not always be the best choice. In cases where there's a large amount of data relative to the number of features, traditional linear regression might perform well without the need for regularization.\n",
    "\n",
    "Non-Linear Relationships: Regularized linear models are inherently linear. If the underlying relationship between the features and the target variable is truly non-linear, these methods might not capture the nuances of the relationship effectively. In such cases, more complex models like decision trees, random forests, or nonlinear regression methods might be more appropriate.\n",
    "\n",
    "Feature Importance: Regularization methods like Lasso can lead to automatic feature selection by driving some coefficients to zero. While this can be beneficial for reducing model complexity, it might discard potentially important features that contribute to predictive accuracy.\n",
    "\n",
    "Model Complexity: In some cases, regularization might not be enough to prevent overfitting if the data is inherently complex or noisy. More advanced techniques, such as ensemble methods (e.g., random forests, gradient boosting), might be needed to handle such complexity effectively.\n",
    "\n",
    "Choice of Regularization Strength: The choice of the regularization strength parameter (alpha) is crucial. A small alpha might not effectively prevent overfitting, while a large alpha might excessively penalize coefficients, leading to underfitting. Choosing the right alpha often requires cross-validation, which can be computationally expensive.\n",
    "\n",
    "Assumption Violation: Regularized linear models assume that the relationship between features and the target is linear. If this assumption is significantly violated, the model might not perform well, and alternative techniques should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccbb35",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "I would chose MAE as the error is less and MAE is robust to ouliers.\n",
    "MAE is another widely used metric that measures the average absolute value of the residuals. It treats all errors equally and does not give more weight to larger errors.\n",
    "MAE is less sensitive to outliers and large errors compared to RMSE.\n",
    "Model B has an MAE of 8, indicating that its predictions deviate from the true values by an average of 8 units.\n",
    "The choice between RMSE and MAE depends on the specific problem and the importance of different errors. RMSE might be more appropriate when larger errors are more critical, while MAE might be preferred when all errors should be treated equally.\n",
    "Both metrics might not be easily interpretable on their own. For better context, it's often useful to compare the chosen metric to a baseline, such as the mean or median of the target variable, to assess how well the model performs relative to a simple prediction method.\n",
    "It's important to keep in mind the units of the target variable when interpreting these metrics. RMSE and MAE are not normalized, so their absolute values might not be directly comparable across different datasets or variables with different scales.\n",
    "In summary, Model B might be preferred based on the provided metrics, but the decision should also consider the specific objectives of the analysis, the nature of the errors, and potential business implications. It's always a good practice to consider multiple metrics and possibly perform additional analysis, such as cross-validation, to get a comprehensive view of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a44968",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "When comparing the performance of two regularized linear models using different types of regularization (Ridge and Lasso), the choice of regularization method depends on the characteristics of the data, the goals of the analysis, and the trade-offs associated with each method. Let's analyze both models and their implications:\n",
    "\n",
    "Model A: Ridge Regularization (α = 0.1)\n",
    "Ridge regression adds the sum of squared coefficients to the cost function, where the regularization parameter α controls the strength of regularization. A smaller α value results in weaker regularization.\n",
    "\n",
    "Model B: Lasso Regularization (α = 0.5)\n",
    "Lasso regression adds the sum of absolute values of coefficients to the cost function. Like Ridge, the regularization parameter α controls the strength of regularization.\n",
    "\n",
    "Choosing the Better Model:\n",
    "The choice between Ridge and Lasso regularization depends on the goals of the analysis and the nature of the data. Here are some considerations:\n",
    "\n",
    "Ridge Regularization:\n",
    "\n",
    "Ridge tends to work well when there are many correlated features, as it distributes the impact of correlated features across them.\n",
    "With a relatively small α (0.1), the regularization effect might not be very strong, and the model might resemble standard linear regression to some extent.\n",
    "Ridge can be more robust when dealing with multicollinearity and when you want to retain all the features but reduce their impact.\n",
    "Lasso Regularization:\n",
    "\n",
    "Lasso is effective for feature selection, as it tends to drive some coefficients to exactly zero. This can simplify the model by excluding irrelevant or redundant features.\n",
    "A higher α (0.5) indicates stronger regularization, potentially leading to more coefficients being set to zero compared to Ridge with α = 0.1.\n",
    "Lasso is particularly useful when you suspect that only a subset of features is relevant for prediction.\n",
    "Trade-Offs and Limitations:\n",
    "\n",
    "Bias-Variance Trade-Off: Regularization methods, including Ridge and Lasso, introduce a bias to the model to reduce variance. As the regularization strength increases, the model becomes simpler and more biased, potentially leading to underfitting.\n",
    "Interpretability: Lasso can lead to sparse models by excluding some features entirely, which might be desirable for feature selection but might also make the model less interpretable.\n",
    "Choice of Regularization Strength: The choice of α is crucial. If α is set too high, the model might underfit; if it's set too low, regularization might not be effective. Cross-validation is often used to find the optimal value.\n",
    "Non-Linearity: Both Ridge and Lasso assume linearity between features and the target. If the relationship is strongly nonlinear, these methods might not perform well.\n",
    "Scaling: Feature scaling is important for both Ridge and Lasso to ensure the regularization is applied uniformly across features.\n",
    "In summary, the choice between Ridge and Lasso regularization depends on the goals of the analysis. If feature selection is a priority and you suspect that only a subset of features are relevant, Lasso might be preferable. If multicollinearity and distributing the impact of correlated features are more important, Ridge might be a better choice. It's recommended to experiment with both methods, adjust regularization strengths, and possibly perform cross-validation to make an informed decision based on your specific data and goals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
